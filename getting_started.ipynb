{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB 4 notbook\n",
    "## LLM testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poly_llm.to_test.file_name_check import file_name_check\n",
    "from poly_llm.common.abstract_executor import AbstractExecutor\n",
    "from poly_llm.common.prompt_generator import PromptGenerator\n",
    "from poly_llm.generators.llm_test_generator import LLMTestGenerator\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "import json\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the code coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = AbstractExecutor(file_name_check)\n",
    "\n",
    "inputs = [\n",
    "    \"example.txt\",\n",
    "    \"1example.dll\",\n",
    "    's1sdf3.asd',\n",
    "    'K.dll',\n",
    "    'MY16FILE3.exe',\n",
    "    'His12FILE94.exe',\n",
    "    '_Y.txt',\n",
    "    '?aREYA.exe',\n",
    "    '/this_is_valid.dll',\n",
    "    'this_is_valid.wow',\n",
    "]\n",
    "\n",
    "# Execute the inputs and print the results\n",
    "for input in inputs:\n",
    "    #exceptions, execution_time, coverage = executor._execute_input(input)\n",
    "    coverage_date = executor._execute_input(input)\n",
    "    print(coverage_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the promt with the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = AbstractExecutor(file_name_check)\n",
    "prompt_generator = PromptGenerator(file_name_check)\n",
    "\n",
    "model_name = \"Salesforce/codet5-large-ntp-py\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) #tokenizer#AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-Python-hf\")#\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name) \n",
    "\n",
    "llm_generator = LLMTestGenerator(model, tokenizer, file_name_check)\n",
    "prompt = prompt_generator.generate_prompt(few_shot_examples=['''def test_file_name_check(): \\n \n",
    "assert file_name_check(\"example.txt\") == 'Yes'  \\n\n",
    "assert file_name_check(\"1example.dll\") == 'No' \\n'''])\n",
    "\n",
    "print(f\"THE PROMPT {prompt}\")\n",
    "test, test_name = llm_generator.create_test_function(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the coverage achived by the LLM produced code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"test_generated.py\"\n",
    "llm_generator.write_test_to_file(test, filename=filename)\n",
    "\n",
    "module_name = filename.split(\".\")[0]\n",
    "function_name = test_name\n",
    "\n",
    "# Dynamically import the module\n",
    "module = importlib.import_module(module_name)\n",
    "function = getattr(module, function_name)\n",
    "\n",
    "executor2 = AbstractExecutor(function)\n",
    "\n",
    "coverage_data = executor2._execute_input(file_name_check)\n",
    "print(coverage_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
